{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fawasafsal/BERT-for-Fake-News-Detection/blob/main/fake_vs_real_news_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F_t4ld7cpej",
        "outputId": "cd20ba33-799f-4b6f-972f-657efd0f4ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfQoYyf7an0J",
        "outputId": "4dc40c88-4337-47bc-c4d6-182164a7fa00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install Required Libraries\n",
        "# Install transformers library for BERT implementation\n",
        "# Install datasets for data handling utilities\n",
        "# Install evaluate for model evaluation metrics\n",
        "# Install scikit-learn for data splitting and metrics\n",
        "# Install pandas for data manipulation\n",
        "\n",
        "!pip install transformers datasets evaluate scikit-learn pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip6GOSHYawuD"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "FrF1v68vbHbC",
        "outputId": "5b3945ea-c842-4d0b-875b-60b20a97b61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your Fake.csv and True.csv files:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b3468c7-685b-4f28-84a1-8258838003f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b3468c7-685b-4f28-84a1-8258838003f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Fake.csv to Fake.csv\n",
            "Saving True.csv to True.csv\n",
            "Loading datasets...\n",
            "Fake news articles: 23481\n",
            "Real news articles: 21417\n",
            "Fake news columns: ['title', 'text', 'subject', 'date']\n",
            "Real news columns: ['title', 'text', 'subject', 'date']\n"
          ]
        }
      ],
      "source": [
        "# Upload and Load Dataset\n",
        "from google.colab import files\n",
        "print(\"Please upload your Fake.csv and True.csv files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the fake news dataset\n",
        "# Fake.csv contain fake news articles\n",
        "# True.csv contain real news articles\n",
        "print(\"Loading datasets...\")\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "\n",
        "# Display information about the datasets\n",
        "print(f\"Fake news articles: {len(df_fake)}\")\n",
        "print(f\"Real news articles: {len(df_real)}\")\n",
        "print(f\"Fake news columns: {df_fake.columns.tolist()}\")\n",
        "print(f\"Real news columns: {df_real.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsrGyEnPbL9U",
        "outputId": "1f5f39a2-5ecf-4578-df00-7be00e505f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total articles: 44898\n",
            "Label distribution:\n",
            "label\n",
            "0    23481\n",
            "1    21417\n",
            "Name: count, dtype: int64\n",
            "Missing values: 0\n",
            "\n",
            "Sample fake news (first 200 chars):\n",
            "Ben Stein Calls Out 9th Circuit Court: Committed a ‘Coup d’état’ Against the Constitution 21st Century Wire says Ben Stein, reputable professor from, Pepperdine University (also of some Hollywood fame\n",
            "\n",
            "Sample real news (first 200 chars):\n",
            "Trump drops Steve Bannon from National Security Council WASHINGTON (Reuters) - U.S. President Donald Trump removed his chief strategist Steve Bannon from the National Security Council on Wednesday, re\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "# Add labels: 0 for fake news, 1 for real news\n",
        "df_fake[\"label\"] = 0  # Fake news labeled as 0\n",
        "df_real[\"label\"] = 1  # Real news labeled as 1\n",
        "\n",
        "# Combine both datasets\n",
        "df = pd.concat([df_fake, df_real])\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Create content column by combining title and text\n",
        "# This gives the model more context for classification\n",
        "df[\"content\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "\n",
        "# Keep only the content and label columns\n",
        "df = df[[\"content\", \"label\"]]\n",
        "\n",
        "# Display dataset statistics\n",
        "print(f\"Total articles: {len(df)}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Display sample articles\n",
        "print(\"\\nSample fake news (first 200 chars):\")\n",
        "print(df[df[\"label\"] == 0][\"content\"].iloc[0][:200])\n",
        "print(\"\\nSample real news (first 200 chars):\")\n",
        "print(df[df[\"label\"] == 1][\"content\"].iloc[0][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Bg8QDjbhfA",
        "outputId": "e86568c9-d0db-48c4-d5b4-927e1117c6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 1000\n",
            "Validation samples: 250\n",
            "Test samples: 250\n"
          ]
        }
      ],
      "source": [
        "# Data Splitting\n",
        "# Split data into train, validation, and test sets\n",
        "# Using smaller subsets for faster training\n",
        "\n",
        "# First split: separate training data from temp data (80/20 split)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"content\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]  # Ensure balanced splits\n",
        ")\n",
        "\n",
        "# Reduce dataset size for faster training and experimentation\n",
        "TRAIN_SIZE = 1000\n",
        "VAL_TEST_SIZE = 500\n",
        "\n",
        "train_texts = train_texts[:TRAIN_SIZE]\n",
        "train_labels = train_labels[:TRAIN_SIZE]\n",
        "\n",
        "# Second split: divide temp data into validation and test sets (50/50 split)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts[:VAL_TEST_SIZE],\n",
        "    temp_labels[:VAL_TEST_SIZE],\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "print(f\"Test samples: {len(test_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "2ecae5cfc0cd43bbba9fa827fce21c45",
            "68da8b8b312544b2a1f930ea344ff5bc",
            "3d66a5a7cbf743308c96b235bfed5ae9",
            "7f1faae16ed4413c9dbcdd1cb086a4e4",
            "181659c4544442b8b169572b7f309074",
            "96d191c3b58f4ef7a185a473347d87b4",
            "fe824dc4bb1a4762b8d70ac49b1e6c7c",
            "6475a9ad7c824d5aaa6fb71a76f14e19",
            "6ef52443b6a24219b4e9773c82a24d98",
            "c4d0f9b77bcf471cabc15a7ddf000e35",
            "3a0e434989c54b70831a785a3068c760",
            "97f44e4ca0774a359559c9f7c570e845",
            "97b2e38866bf42808d075c5cb67c9ccf",
            "2d05a3a1743f4abf8e3d5531dda758e4",
            "048594889046429593042bdfc35862c3",
            "96ee3761d1104a7c8e7271df049298bf",
            "18e8cc505565407db701ea6b9d8e83b2",
            "b3e7440cd8b043919e7b36ba35d8b727",
            "71208b20e35040eb9c6f08efdc9ca427",
            "776026adb064463babe30bbbd71ac2ff",
            "39231540306a48fbad319b1e792727d1",
            "f5175e0fcd514583bbf274b7899baa0f",
            "0b31102b5b08466a860300caef7d09b3",
            "59513fed062142408df31cbf04d927a9",
            "008bc7fe80874b8ab2e2c6173bf9d635",
            "7fa0d6abb00341039b08120fbc94e2c0",
            "f733ca4536de4946ab61e1f1ca405786",
            "a6a88edd77634381b2de08ec22fd22ba",
            "f8d1480dff3a4ded8efc433d99e485e4",
            "6b2bc9f8b8c84e209a9f4a5dcdf8a6c7",
            "2231323510b347958fa58ed57baa1ff2",
            "61fcca2bed6d4e01bc2f6149df88b294",
            "7cff9885f76143ba911fdae0c2cd4d38",
            "e43dee5bd3bb40909a1f7a868584764d",
            "0f196c3fb67644198df303f344e72377",
            "b760dc944e0647208f1b79b4cb832455",
            "27072d8de30f4913b4a69b7e26059631",
            "96c074bf4cec4b6dbc7189f35abe250a",
            "31f40d9ee2c3484593a7b852f37dfb28",
            "69c0a5993e354d2db3c448c1e69ae100",
            "641132daa238417fb0a62a1420108c2c",
            "b0a414988ef7465cb8fc4befcf4cbaf3",
            "b28d53cf53ad4417bc346a8971edf71e",
            "32f32cdf115c405fa9d87db7c2aecfa9"
          ]
        },
        "id": "TLzifcCybprq",
        "outputId": "81d61432-5f64-42f5-deca-fa50f80a7d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ecae5cfc0cd43bbba9fa827fce21c45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97f44e4ca0774a359559c9f7c570e845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b31102b5b08466a860300caef7d09b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e43dee5bd3bb40909a1f7a868584764d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer vocabulary size: 30522\n",
            "Max model input length: 512\n"
          ]
        }
      ],
      "source": [
        "# Initialize BERT Tokenizer\n",
        "# Load BERT tokenizer for text preprocessing\n",
        "# bert-base-uncased: lowercase, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Max model input length: {tokenizer.model_max_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv-zRa-gbzdt",
        "outputId": "36c426d0-4b8b-4555-8103-21bff291c9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing training data...\n",
            "Tokenizing validation data...\n",
            "Tokenizing test data...\n",
            "Training encoding shape: torch.Size([1000, 128])\n",
            "Validation encoding shape: torch.Size([250, 128])\n",
            "Test encoding shape: torch.Size([250, 128])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize Text Data\n",
        "# Convert text to tokens that BERT can understand\n",
        "# truncation=True: Cut text if longer than max_length\n",
        "# padding=True: Pad shorter sequences to max_length\n",
        "# max_length=128: Reduced from 512 for faster training\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "print(\"Tokenizing training data...\")\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation data...\")\n",
        "val_encodings = tokenizer(\n",
        "    val_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Tokenizing test data...\")\n",
        "test_encodings = tokenizer(\n",
        "    test_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"Training encoding shape: {train_encodings['input_ids'].shape}\")\n",
        "print(f\"Validation encoding shape: {val_encodings['input_ids'].shape}\")\n",
        "print(f\"Test encoding shape: {test_encodings['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aotc0kMVb7SP",
        "outputId": "95cb3eea-9de6-4187-b0d9-d8947a445739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 1000\n",
            "Validation dataset size: 250\n",
            "Test dataset size: 250\n"
          ]
        }
      ],
      "source": [
        "# Create PyTorch Dataset Class\n",
        "# Custom dataset class to handle tokenized data for PyTorch training\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for handling tokenized news articles\n",
        "    \"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        \"\"\"\n",
        "        Initialize dataset with tokenized encodings and labels\n",
        "\n",
        "        Args:\n",
        "            encodings: Dictionary containing tokenized text data\n",
        "            labels: List of binary labels (0 for fake, 1 for real)\n",
        "        \"\"\"\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing input tensors and labels\n",
        "        \"\"\"\n",
        "        # Extract tokenized data for the given index\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        # Add the corresponding label\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962,
          "referenced_widgets": [
            "736703f2498445c8bb7db2eb0e61e8f3",
            "92fe8b33b83a45c9b498c60bc8bd4e38",
            "b26c72f55af449bdb649ed15473092c4",
            "567b31dcbf1e461f9a10a21fcd514611",
            "06c8c17e16a442d9b3bd233f879de3b4",
            "062063fd8e5f4cb8941c8be2ed588cf0",
            "b202ccbc487f4a85a8b79c7d4e2b93e4",
            "0e11ae2ebbca4cfd9c495e17aaf15d39",
            "2b9cb569af4f4d1b86274324cce6102d",
            "5110e94df2184529b65d6792ea904bd7",
            "d2543cc2db8e43c681e720022bb491e0"
          ]
        },
        "id": "acoQiblnb_bn",
        "outputId": "3c2e1f14-5323-4ba0-9db4-90c5903eacff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "736703f2498445c8bb7db2eb0e61e8f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model type: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
            "Number of parameters: 109,483,778\n",
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Pre-trained BERT Model\n",
        "# Load BERT model for sequence classification\n",
        "# num_labels=2: Binary classification (fake vs real)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2  # Binary classification\n",
        ")\n",
        "\n",
        "# Display model information\n",
        "print(f\"Model type: {type(model)}\")\n",
        "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISKhhM1ncHUk"
      },
      "outputs": [],
      "source": [
        "# Define Evaluation Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for the model\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Tuple containing predictions and true labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing computed metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    # Get predicted class (argmax of logits)\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzULfW-3cIm8",
        "outputId": "c0e5c904-1105-4343-ae11-29b6c0847206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training arguments configured:\n",
            "- Batch size: 8\n",
            "- Learning rate: 2e-05\n",
            "- Epochs: 3\n",
            "- Warmup steps: 100\n"
          ]
        }
      ],
      "source": [
        "# Configure Training Arguments\n",
        "# Disable wandb logging to avoid authentication issues\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Define training hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results-news\",              # Directory to save model checkpoints\n",
        "    per_device_train_batch_size=8,           # Batch size for training\n",
        "    per_device_eval_batch_size=8,            # Batch size for evaluation\n",
        "    num_train_epochs=3,                      # Number of training epochs\n",
        "    learning_rate=2e-5,                      # Learning rate (typical for BERT)\n",
        "    warmup_steps=100,                        # Number of warmup steps\n",
        "    weight_decay=0.01,                       # Weight decay for regularization\n",
        "    logging_dir=\"./logs\",                    # Directory for storing logs\n",
        "    logging_steps=50,                        # Log every 50 steps\n",
        "    save_steps=200,                          # Save every 200 steps\n",
        "    seed=42                                  # Set seed for reproducibility\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured:\")\n",
        "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"- Warmup steps: {training_args.warmup_steps}\")\n",
        "\n",
        "# Alternative approach for evaluation during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ATaQotBRerUO",
        "outputId": "362e2b9e-f099-4cb2-9601-abd4fb983473"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-13-1320347649.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "==================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-983452145.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='312' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/375 14:43 < 02:59, 0.35 it/s, Epoch 2.49/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-983452145.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 17:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed!\n",
            "Final training loss: 0.1171\n",
            "\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-983452145.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "==============================\n",
            "eval_loss: 0.0006\n",
            "eval_accuracy: 1.0000\n",
            "eval_f1: 1.0000\n",
            "eval_precision: 1.0000\n",
            "eval_recall: 1.0000\n",
            "eval_runtime: 26.2169\n",
            "eval_samples_per_second: 9.5360\n",
            "eval_steps_per_second: 1.2210\n",
            "epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "# Initialize Trainer and Start Training\n",
        "# Create Trainer instance with model, training arguments, and datasets\n",
        "trainer = Trainer(\n",
        "    model=model,                            # The pre-trained BERT model\n",
        "    args=training_args,                     # Training configuration\n",
        "    train_dataset=train_dataset,            # Training data\n",
        "    eval_dataset=val_dataset,               # Validation data\n",
        "    compute_metrics=compute_metrics,        # Evaluation metrics function\n",
        "    tokenizer=tokenizer                     # Tokenizer for text processing\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Start the training process\n",
        "training_results = trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
        "\n",
        "# Evaluate on validation set after training\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "val_results = trainer.evaluate(val_dataset)\n",
        "\n",
        "print(\"Validation Results:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in val_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "3dRO5Mc-jWx2",
        "outputId": "0bf2bfab-08c9-4aa4-d08e-3531fc2c5b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-983452145.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 01:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results:\n",
            "==============================\n",
            "eval_loss: 0.0368\n",
            "eval_accuracy: 0.9920\n",
            "eval_f1: 0.9920\n",
            "eval_precision: 0.9920\n",
            "eval_recall: 0.9920\n",
            "eval_runtime: 27.8325\n",
            "eval_samples_per_second: 8.9820\n",
            "eval_steps_per_second: 1.1500\n",
            "epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on Test Set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Results:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in test_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVESwTDbjisL",
        "outputId": "d60c0baf-5968-4baa-9b13-5d21f40686ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Predictions:\n",
            "========================================\n",
            "Text 1: Breaking: Scientists discover new species of dinosaur in remote jungle expedition....\n",
            "Prediction: FAKE (Confidence: 0.9981)\n",
            "----------------------------------------\n",
            "Text 2: SHOCKING: Aliens have been secretly controlling world governments for decades, insider reveals!...\n",
            "Prediction: FAKE (Confidence: 0.9985)\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions on Sample Data\n",
        "def predict_news(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Predict whether a news article is fake or real\n",
        "\n",
        "    Args:\n",
        "        text: News article text\n",
        "        model: Trained BERT model\n",
        "        tokenizer: BERT tokenizer\n",
        "        device: Computing device (CPU/GPU)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (prediction, confidence_score)\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
        "\n",
        "    # Make prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Test with sample articles\n",
        "sample_texts = [\n",
        "    \"Breaking: Scientists discover new species of dinosaur in remote jungle expedition.\",\n",
        "    \"SHOCKING: Aliens have been secretly controlling world governments for decades, insider reveals!\"\n",
        "]\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"=\" * 40)\n",
        "for i, text in enumerate(sample_texts):\n",
        "    prediction, confidence = predict_news(text, model, tokenizer, device)\n",
        "    label = \"REAL\" if prediction == 1 else \"FAKE\"\n",
        "    print(f\"Text {i+1}: {text[:100]}...\")\n",
        "    print(f\"Prediction: {label} (Confidence: {confidence:.4f})\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZe_SqZhjl03",
        "outputId": "c77f95ec-d1bd-4e33-c7be-c1a75ed60b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model saved to: ./fine-tuned-bert-news-classifier\n",
            "You can load this model later using:\n",
            "model = AutoModelForSequenceClassification.from_pretrained('./fine-tuned-bert-news-classifier')\n",
            "tokenizer = AutoTokenizer.from_pretrained('./fine-tuned-bert-news-classifier')\n"
          ]
        }
      ],
      "source": [
        "# Save the Fine-tuned Model\n",
        "# Save the model and tokenizer for future use\n",
        "model_save_path = \"./fine-tuned-bert-news-classifier\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"\\nModel saved to: {model_save_path}\")\n",
        "print(\"You can load this model later using:\")\n",
        "print(f\"model = AutoModelForSequenceClassification.from_pretrained('{model_save_path}')\")\n",
        "print(f\"tokenizer = AutoTokenizer.from_pretrained('{model_save_path}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ur8D5fMjqTF",
        "outputId": "664caaba-d3eb-4bdd-b2b0-395e7e3f6fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING SUMMARY\n",
            "============================================================\n",
            "✓ Model: BERT-base-uncased\n",
            "✓ Task: Binary classification (Fake vs Real news)\n",
            "✓ Training samples: 1000\n",
            "✓ Validation samples: 250\n",
            "✓ Test samples: 250\n",
            "✓ Training epochs: 3\n",
            "✓ Final test accuracy: 0.9920\n",
            "✓ Final test F1-score: 0.9920\n"
          ]
        }
      ],
      "source": [
        "# Training Summary\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"✓ Model: BERT-base-uncased\")\n",
        "print(f\"✓ Task: Binary classification (Fake vs Real news)\")\n",
        "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
        "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
        "print(f\"✓ Test samples: {len(test_dataset)}\")\n",
        "print(f\"✓ Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"✓ Final test accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"✓ Final test F1-score: {test_results['eval_f1']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}